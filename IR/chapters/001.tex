\chapter{Introduction to Information Retrieval}

\section{Introduction}

\paragraph{Predictive Text Analysis} a computer program that automatically recognize or detect \textbf{a particular concept} within a text.

\paragraph{Explorative Text Analysis} a computer program that discovers \textbf{patterns or useful trends} in a collection of texts.

\paragraph{Opinion Mining} a program that detects whether an opinionated text expresses a positive or negative opinion over an argument.

\paragraph{Sentiment Analysis} a program that detects emotional state of the author of a text.

\paragraph{Bias Detection} a program that detects whether the author of a text favors a specific viewpoint.

\paragraph{Information Extraction} detects that some words refer to a specific entity within a text.

\paragraph{Relation Learning} detects that some relation instates between two entities in a text.

\paragraph{Text-driven Forecasting} monitors incoming text (as stream) making predictions about events or properties.

\paragraph{Temporal Summarization} summaries a stream of text.

\paragraph{Structured Text} Formal languages often have a structure which has some semantic relevance and meaningful data can be extracted within this structure (the abstract field of a paper).

\section{Basic Text Processing}

\paragraph{Document} The unit of textual resource is the \textbf{word} which may have a structure (logical or physical, or whatever), some medias, and some metadata.
A word is meaningful data, an index is used to retrieve faster or link documents.

\paragraph{Descriptive Metadata} Is related to the document as unit (TODO: example).

\paragraph{Semantic Metadata} Is related to the content of the document (TODO: example).

\subsection{Text Pre-Processing}

\paragraph{How to represent a text?} tokenization (and lemmization) of text. The units of informations should be words, since sentences or whole documents don't offer sufficient granularity in order to extract meaningful entities and relations. Words are the basic "features" of the text (leads to Word Embedding).

\paragraph{Tokenization} splitting a stream of characters into unit of information (lexing rules strictly depends on the language, pictograms != wordlings != soundcarriers).
Each token is a candidate for being a meaningful terms (index, key ...).

\paragraph{Normalization} establishing rules in order to have the same notation for the same entity (ex: U.s.A -> USA).

\paragraph{Stemming} reducing variations of words into the same form (ex: authorization -> authorize), but is optional.

\paragraph{Noise removal} erasing useless or noisy words (ex: conjunctions, articles, prepositions, punctuation), optional.
It may be an advantage when dealing with the implementation of a search engines (reduces the size of the language of keys).

\paragraph{Types and Tokens} just as in lexers, the type is an abstract kind of token, while the latter is just an instance of a type.

The \textit{vocabulary} is constructed on the corpora, it's useless to use a huge vocabulary from the entire language X.

\paragraph{Heaps/Herdan's law} There's a general empiric law for size of vocabulary $|V| = kN^\beta$, where $0.67 < \beta < 0.75$, and $N$ is the number of words in the text.

\paragraph{Corpora} words are part of a context which is the corpus (further the corpora), and has an author, a time/space location, a target language/audience, a function. A document may be written in more than one language/slang. with or without junctions.

\paragraph{Segmentation of Sentences} identification of boundaries of sentences and text with punctuation.

\paragraph{Clitic} a word that is represented with punctuation and other symbols (ex: \textit{we're} contains implicit \text{are}).

\paragraph{Language issues} in complex languages like German, as well as when using pictograms, dividing words with a compound splitter is useful.
Some languages also don't use space or punctuation as separation of words or sentences. Inaccurate splitting may end changing the meaning of the sentences.

\subsection{Word Tokenization}

\paragraph{Alternative tokenization methods}
\begin{itemize}
  \item Byte Pair Encoding
  \item Unigram language modeling tokenization
  \item WordPiece
\end{itemize}

All of these are statistical methods and have a \textbf{token learner} which learns the vocabulary from the data, and the \textbf{token segmenter} which applies this vocabulary to a random stream of text.
Also regular expressions could be used to lex tokens, as done with compilers.
