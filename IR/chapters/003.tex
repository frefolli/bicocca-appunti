\chapter{POS, NER, NLP}

\section{Part of Speech Tagging / Named Entity Recognition}

\paragraph{POS}

Marking words in a text with the role in that sentence / text (ex: "chasing" -> "Verb", is" -> "auxiliary").
<<Word classes may be classified as open or closed: open classes (typically including nouns, verbs and adjectives) acquire new members constantly, while closed classes (such as pronouns and conjunctions) acquire new members infrequently, if at all>> \cite{enwiki:1251632448}.

We need a reference for tagging: Penn Treebank has a set of tags which annotate a corpus.
The may be some ambiguity within a tense due to context which influences the meaning and role of a term.
Well, many words can only be assigned to only one TAG, others have a most-probable TAG which is by far the best.
There may be also correlations between a term being a TAG and another TAG/word appearing nearby.
Ambiguity is thus solved with probability and statistics.

\section{Statistical Language Model}

A Language Model is a kind of representation for a text (based on probability distributions) by modelling a sequence of words as an event and assigning a probability to it. If such model is used to generate new data from a collection, then it's called also Generative Model.
The probability for a sentence is $P(Ws) = P(w_1,w_2...w_n)$ and the probability of an upcoming word (Prediction) is $P(w_{n+1}|w_1,w_2...w_n)$. A model which computes such probabilities is thus called \emph{Language Model}.
An \emph{N-gram} is a tuple $(w_n, (w_1, w_2 ... w_{n-1}))$ where i compute probabilities as $P(w_n|w_1, w_2 ... w_{n-1})$.
A Language Model is \emph{well-formed} over an alphabet $Sigma$ iff $\Sigma P(S) = 1$ with $S \in \Sigma^{*}$.
With the \emph{chain rule} we apply $P(w_n|w_1, w_2 ... w_{n-1}) = P(w_n|w_{n-1}, w_{n-2}) * P(w_{n-1}|w_{n-2},w_{w-3})) ... etc$.

When using a Language Model, a different model is used for each document.

\paragraph{Smoothing} Is a technique in which all the probabilities of words seen in the document are lowered by some amount in order to create a difference from the total probability of the words and $1$. This difference is reassigned (equally divided) to the words which didn't appeared. In such a way there aren't $P(W) = 0$ for any word $W$ (preventing zero denominator in the maximum likelihood estimation formula $P(w_i | w_{i-1}) = \frac {count(w_{i-1}, w_i)} {count(w_{i-1})}$).

\paragraph{Perplexity} is the multiplicative inverse of the probability given by a language model on a test set which has not been seen before. It's useful to evaluate multiple language models. A language model which is less surprised (perplexed) is the one which is more accurate. In the formula there is a normalization factor (which applies the root-by-N) where N is the number of words.
For obvious reasons, n-grams with more N are the ones with less perplexity.
