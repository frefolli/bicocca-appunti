\chapter{Text Representation}

\section{Basics}

\paragraph{Incidence Matrices}

The most simple representation is the \textbf{bag of words}. Putting on more formal representations, we see the \textbf{Incidence Matrix} in which the \textit{columns} are the documents, and the \textit{rows} are the words in the vocabulary. Each cell represents if a word appears in the document, or the number of occurrences of such word in the document (\textbf{Count Matrix}). Of course this matrix should be a sparse matrix.

\paragraph{Bag of Words with N-grams}

An \textbf{N-gram} is a continuous sequence of N tokens from a document (ex: "Sistema di Controllo Marcia Treno", 2-grams are: "Sistema_di", "di_Controllo", "Controllo_Marcia", "Marcia_Treno").
This concept is somehow used also in computational genetics to represent alignments, because this way is possible to retain information about relative position of words.
Of course it comes with the cost of space and parametricity.

\paragraph{Statistics}

The \textit{Zipf's law} says that the product of frequency $f(W)$ and rank order $r(w)$ of words is somewhat constant. So one could write $f(w) = \frac 1 {r(w)} = \frac K {f(w)}$ for some arbitrary $K$ constant value.
